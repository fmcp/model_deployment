{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ow23zCFEH4v"
      },
      "source": [
        "# Ejemplo de Quantization\n",
        "---\n",
        "\n",
        "En este ejemplo vamos a ver como cambiar la representación del modelo pasando los pesos y activaciones de FP32 a INT8. De esta forma, se obtinenen dos beneficios potenciales:\n",
        "\n",
        "\n",
        "1.   Reducimos el tamaño que ocupa el modelo ya que los pesos ocupan una cuarta parte (8 bits vs 32 bits por peso).\n",
        "2.   Si el dispositivo incorpora hardware para trabajar en 8 bits, se reduce el tiempo de ejecución. Sino, se mantiene el mismo que para 32 bits.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Instalar e importar las librerías necesarias\n",
        "\n",
        "En este ejemplo vamos a trabajar con Pytorch y los modelos de torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnsq0y32D6KW",
        "outputId": "d79b8a49-b881-42cc-f1cd-9229674b3632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.12/dist-packages (1.8.0)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (1.20.1)\n",
            "Requirement already satisfied: onnxruntime-gpu in /usr/local/lib/python3.12/dist-packages (1.24.2)\n",
            "Requirement already satisfied: tensorrt in /usr/local/lib/python3.12/dist-packages (10.15.1.29)\n",
            "Collecting onnxscript\n",
            "  Downloading onnxscript-0.6.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.6)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.4)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (25.12.19)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (26.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu) (1.14.0)\n",
            "Requirement already satisfied: tensorrt-cu13==10.15.1.29 in /usr/local/lib/python3.12/dist-packages (from tensorrt) (10.15.1.29)\n",
            "Requirement already satisfied: tensorrt-cu13-libs==10.15.1.29 in /usr/local/lib/python3.12/dist-packages (from tensorrt-cu13==10.15.1.29->tensorrt) (10.15.1.29)\n",
            "Requirement already satisfied: tensorrt-cu13-bindings==10.15.1.29 in /usr/local/lib/python3.12/dist-packages (from tensorrt-cu13==10.15.1.29->tensorrt) (10.15.1.29)\n",
            "Collecting onnx_ir<2,>=0.1.15 (from onnxscript)\n",
            "  Downloading onnx_ir-0.1.16-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
            "Downloading onnxscript-0.6.2-py3-none-any.whl (689 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m689.1/689.1 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx_ir-0.1.16-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.3/159.3 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx_ir, onnxscript\n",
            "Successfully installed onnx_ir-0.1.16 onnxscript-0.6.2\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torchinfo onnx onnxruntime-gpu tensorrt onnxscript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QcTQd3QwEM5j"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import alexnet, AlexNet_Weights\n",
        "from torchinfo import summary\n",
        "import torch\n",
        "import torchvision\n",
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk6e0K0QGkOa"
      },
      "source": [
        "## 2. Definir el modelo\n",
        "\n",
        "Definimos el modelo, en este caso, usamos AlexNet pre-entrenada en ImageNet. Usamos esta red ya que es una red lineal sin conexiones residuales que producen problemas con la cuantización. Este tipo de problemas se pueden solventar cambiando algunas operaciones del modelo como se ve en este ejemplo para ResNet50 (https://github.com/zanvari/resnet50-quantization/blob/main/quantization-resnet50.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kPW7a7CGAIm",
        "outputId": "3ca3ba02-b5ed-45e4-a4dc-d97f6f4180f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233M/233M [00:01<00:00, 183MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Mult-Adds\n",
              "============================================================================================================================================\n",
              "AlexNet                                  [1, 3, 224, 224]          [1, 1000]                 --                        --\n",
              "├─Sequential: 1-1                        [1, 3, 224, 224]          [1, 256, 6, 6]            --                        --\n",
              "│    └─Conv2d: 2-1                       [1, 3, 224, 224]          [1, 64, 55, 55]           23,296                    70,470,400\n",
              "│    └─ReLU: 2-2                         [1, 64, 55, 55]           [1, 64, 55, 55]           --                        --\n",
              "│    └─MaxPool2d: 2-3                    [1, 64, 55, 55]           [1, 64, 27, 27]           --                        --\n",
              "│    └─Conv2d: 2-4                       [1, 64, 27, 27]           [1, 192, 27, 27]          307,392                   224,088,768\n",
              "│    └─ReLU: 2-5                         [1, 192, 27, 27]          [1, 192, 27, 27]          --                        --\n",
              "│    └─MaxPool2d: 2-6                    [1, 192, 27, 27]          [1, 192, 13, 13]          --                        --\n",
              "│    └─Conv2d: 2-7                       [1, 192, 13, 13]          [1, 384, 13, 13]          663,936                   112,205,184\n",
              "│    └─ReLU: 2-8                         [1, 384, 13, 13]          [1, 384, 13, 13]          --                        --\n",
              "│    └─Conv2d: 2-9                       [1, 384, 13, 13]          [1, 256, 13, 13]          884,992                   149,563,648\n",
              "│    └─ReLU: 2-10                        [1, 256, 13, 13]          [1, 256, 13, 13]          --                        --\n",
              "│    └─Conv2d: 2-11                      [1, 256, 13, 13]          [1, 256, 13, 13]          590,080                   99,723,520\n",
              "│    └─ReLU: 2-12                        [1, 256, 13, 13]          [1, 256, 13, 13]          --                        --\n",
              "│    └─MaxPool2d: 2-13                   [1, 256, 13, 13]          [1, 256, 6, 6]            --                        --\n",
              "├─AdaptiveAvgPool2d: 1-2                 [1, 256, 6, 6]            [1, 256, 6, 6]            --                        --\n",
              "├─Sequential: 1-3                        [1, 9216]                 [1, 1000]                 --                        --\n",
              "│    └─Dropout: 2-14                     [1, 9216]                 [1, 9216]                 --                        --\n",
              "│    └─Linear: 2-15                      [1, 9216]                 [1, 4096]                 37,752,832                37,752,832\n",
              "│    └─ReLU: 2-16                        [1, 4096]                 [1, 4096]                 --                        --\n",
              "│    └─Dropout: 2-17                     [1, 4096]                 [1, 4096]                 --                        --\n",
              "│    └─Linear: 2-18                      [1, 4096]                 [1, 4096]                 16,781,312                16,781,312\n",
              "│    └─ReLU: 2-19                        [1, 4096]                 [1, 4096]                 --                        --\n",
              "│    └─Linear: 2-20                      [1, 4096]                 [1, 1000]                 4,097,000                 4,097,000\n",
              "============================================================================================================================================\n",
              "Total params: 61,100,840\n",
              "Trainable params: 61,100,840\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 714.68\n",
              "============================================================================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 3.95\n",
              "Params size (MB): 244.40\n",
              "Estimated Total Size (MB): 248.96\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "model = alexnet(AlexNet_Weights)\n",
        "preprocessing = AlexNet_Weights.IMAGENET1K_V1.transforms()\n",
        "summary(model, input_size=(1, 3, 224, 224), col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEdYSwpwM2gn"
      },
      "source": [
        "## 3. Definir un data loader\n",
        "\n",
        "Por limitaciones de tiempo de cómputo, vamos a trabajar con CIFAR-10 pero cualquier dataset es válido. Primero, tenemos que crear un DataLoader de Pytorch para poder usar los datos con nuestro modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UN8vwaeNb6tU",
        "outputId": "5670d7a8-9072-4824-d099-44b722b07164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 30.7MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "dataset = torchvision.datasets.CIFAR10(root='./cifar10', train=True, transform=preprocessing, download=True)\n",
        "train_data_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ihHzo0DO9SL"
      },
      "source": [
        "## 4. Preparar la cuantización\n",
        "\n",
        "En este ejemplo vamos a usar una 'Quantization-Aware Training' para calibrar y transformar los pesos y activaciones de FP32 a INT8. De esta forma, los pesos se adaptan al nuevo rango de representación evitando problemas de cálculos que se salen fuera de rango y obteniendo un mejor accruacy que usando otras técnicas de cuantización como el 'Post-Training Quantization'.\n",
        "\n",
        "Para ello, tenemos que añadir unos adaptadores a la entrada y salida del modelo para convertir las entradas de FP32 a INT8 y nuestras salidas de INT8 a FP32. Tras esto, definimos la librería que realizará la cuantización y que depende del hardware en el que vamos a desplegar. Pytorch ofrece las siguientes opciones: https://pytorch.org/docs/stable/quantization.html#backend-hardware-support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIX9cfdoLPF4",
        "outputId": "e6298c97-661a-4359-ea72-934382e73112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4148835348.py:4: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_fp32_prepared = torch.ao.quantization.prepare_qat(model_fp32)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "================================================================================================================================================================\n",
              "Layer (type:depth-idx)                                       Input Shape               Output Shape              Param #                   Mult-Adds\n",
              "================================================================================================================================================================\n",
              "Sequential                                                   [1, 3, 224, 224]          [1, 1000]                 --                        --\n",
              "├─QuantStub: 1-1                                             [1, 3, 224, 224]          [1, 3, 224, 224]          --                        --\n",
              "│    └─FusedMovingAvgObsFakeQuantize: 2-1                    [1, 3, 224, 224]          [1, 3, 224, 224]          --                        --\n",
              "├─AlexNet: 1-2                                               [1, 3, 224, 224]          [1, 1000]                 --                        --\n",
              "│    └─Sequential: 2-2                                       [1, 3, 224, 224]          [1, 256, 6, 6]            --                        --\n",
              "│    │    └─Conv2d: 3-1                                      [1, 3, 224, 224]          [1, 64, 55, 55]           23,296                    0\n",
              "│    │    └─ReLU: 3-2                                        [1, 64, 55, 55]           [1, 64, 55, 55]           --                        --\n",
              "│    │    └─MaxPool2d: 3-3                                   [1, 64, 55, 55]           [1, 64, 27, 27]           --                        --\n",
              "│    │    └─Conv2d: 3-4                                      [1, 64, 27, 27]           [1, 192, 27, 27]          307,392                   0\n",
              "│    │    └─ReLU: 3-5                                        [1, 192, 27, 27]          [1, 192, 27, 27]          --                        --\n",
              "│    │    └─MaxPool2d: 3-6                                   [1, 192, 27, 27]          [1, 192, 13, 13]          --                        --\n",
              "│    │    └─Conv2d: 3-7                                      [1, 192, 13, 13]          [1, 384, 13, 13]          663,936                   0\n",
              "│    │    └─ReLU: 3-8                                        [1, 384, 13, 13]          [1, 384, 13, 13]          --                        --\n",
              "│    │    └─Conv2d: 3-9                                      [1, 384, 13, 13]          [1, 256, 13, 13]          884,992                   0\n",
              "│    │    └─ReLU: 3-10                                       [1, 256, 13, 13]          [1, 256, 13, 13]          --                        --\n",
              "│    │    └─Conv2d: 3-11                                     [1, 256, 13, 13]          [1, 256, 13, 13]          590,080                   0\n",
              "│    │    └─ReLU: 3-12                                       [1, 256, 13, 13]          [1, 256, 13, 13]          --                        --\n",
              "│    │    └─MaxPool2d: 3-13                                  [1, 256, 13, 13]          [1, 256, 6, 6]            --                        --\n",
              "│    └─AdaptiveAvgPool2d: 2-3                                [1, 256, 6, 6]            [1, 256, 6, 6]            --                        --\n",
              "│    └─Sequential: 2-4                                       [1, 9216]                 [1, 1000]                 --                        --\n",
              "│    │    └─Dropout: 3-14                                    [1, 9216]                 [1, 9216]                 --                        --\n",
              "│    │    └─Linear: 3-15                                     [1, 9216]                 [1, 4096]                 37,752,832                0\n",
              "│    │    └─ReLU: 3-16                                       [1, 4096]                 [1, 4096]                 --                        --\n",
              "│    │    └─Dropout: 3-17                                    [1, 4096]                 [1, 4096]                 --                        --\n",
              "│    │    └─Linear: 3-18                                     [1, 4096]                 [1, 4096]                 16,781,312                0\n",
              "│    │    └─ReLU: 3-19                                       [1, 4096]                 [1, 4096]                 --                        --\n",
              "│    │    └─Linear: 3-20                                     [1, 4096]                 [1, 1000]                 4,097,000                 0\n",
              "├─DeQuantStub: 1-3                                           [1, 1000]                 [1, 1000]                 --                        --\n",
              "================================================================================================================================================================\n",
              "Total params: 61,100,840\n",
              "Trainable params: 61,100,840\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0\n",
              "================================================================================================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.60\n",
              "================================================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "model_fp32 = torch.nn.Sequential(torch.ao.quantization.QuantStub(), model, torch.ao.quantization.DeQuantStub())\n",
        "model_fp32.train()\n",
        "model_fp32.qconfig = torch.ao.quantization.get_default_qat_qconfig('fbgemm')\n",
        "model_fp32_prepared = torch.ao.quantization.prepare_qat(model_fp32)\n",
        "summary(model_fp32_prepared, input_size=(1, 3, 224, 224), col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPFQTzjQQo7Q"
      },
      "source": [
        "## 5. Entrenamiento del modelo\n",
        "\n",
        "Realizamos unas épocas para calibrar los pesos del modelo y adaptarlo a la nueva representación. Para ello, usamos la base de datos que hemos descargado en el punto 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzVUI780Mx8s",
        "outputId": "f798c27d-6902-4f4e-ee9c-c80f365583a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Summary for epoch 0: loss: 1.17e+07, acc: 9.99]  time: 121.802s **\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 1\n",
        "opt = torch.optim.Adam(model_fp32_prepared.parameters(), lr=0.001)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "model_fp32_prepared.train().to('cuda')\n",
        "for epoch in range(n_epochs): # Entrenamos n epocas\n",
        "    train_running_loss = 0.0\n",
        "    train_running_correct = 0\n",
        "    counter = 0\n",
        "    time_start = time.time()\n",
        "    for inputs, labels in train_data_loader: # Obtenemos todos los batch de entrenamiento y los usamos para entrenar\n",
        "        inputs = inputs.to('cuda')\n",
        "        labels = labels.to('cuda')\n",
        "        opt.zero_grad()\n",
        "        outs = model_fp32_prepared(inputs)\n",
        "        loss = loss_fn(outs, labels)\n",
        "        train_running_loss += loss.item()\n",
        "        _, preds = torch.max(outs.data, 1)\n",
        "        train_running_correct += (preds == labels).sum().item()\n",
        "        counter = counter + 1\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "    epoch_loss = train_running_loss / counter\n",
        "    epoch_acc = 100. * (train_running_correct / len(train_data_loader.dataset))\n",
        "    time_end = time.time() - time_start\n",
        "    print(f'** Summary for epoch {epoch}: '\n",
        "\t\tf'loss: {epoch_loss:#.3g}, acc: {epoch_acc:#.3g}]  '\n",
        "\t\tf'time: {time_end:.3f}s **')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxGT16QHQ6a0"
      },
      "source": [
        "## 6. Exportar el modelo en INT8\n",
        "\n",
        "Una vez que hemos realizado el entrenamiento para pasar a INT8, simplemente limpiamos las capas auxiliares que añade Pytorch para realizar la calibración y exportamos el modelo a TorchScript para poder usarlo en un móvil."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "JTIDTKCLidSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe6319b5-8854-4395-c308-963921edb140"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3856809601.py:3: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_int8 = torch.ao.quantization.convert(model_fp32_prepared, inplace=False)\n",
            "W0219 18:55:14.036000 374 torch/onnx/_internal/exporter/_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 13 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exportando el modelo preparado para QAT a ONNX...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0219 18:55:14.606000 374 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
            "W0219 18:55:14.608000 374 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
            "W0219 18:55:14.610000 374 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n",
            "W0219 18:55:14.613000 374 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.onnx] Obtain model graph for `Sequential([...]` with `torch.export.export(..., strict=False)`...\n",
            "[torch.onnx] Obtain model graph for `Sequential([...]` with `torch.export.export(..., strict=False)`... ❌\n",
            "[torch.onnx] Obtain model graph for `Sequential([...]` with `torch.export.export(..., strict=True)`...\n",
            "[torch.onnx] Obtain model graph for `Sequential([...]` with `torch.export.export(..., strict=True)`... ❌\n",
            "[torch.onnx] Falling back to legacy torch.onnx.export due to the following error: Failed to export the model with torch.export. \u001b[96mThis is step 1/3\u001b[0m of exporting the model to ONNX. Next steps:\n",
            "- Modify the model code for `torch.export.export` to succeed. Refer to https://pytorch.org/docs/stable/generated/exportdb/index.html for more information.\n",
            "- Debug `torch.export.export` and submit a PR to PyTorch.\n",
            "- Create an issue in the PyTorch GitHub repository against the \u001b[96m*torch.export*\u001b[0m component and attach the full error stack as well as reproduction scripts.\n",
            "\n",
            "## Exception summary\n",
            "\n",
            "<class 'AttributeError'>: __torch__.torch.classes.quantized.Conv2dPackedParamsBase (of Python compilation unit at: 0) does not have a field with name '__obj_flatten__'\n",
            "\n",
            "(Refer to the full stack trace above for more information.)\n",
            "Modelo exportado a alexnet_qat_int8.onnx exitosamente.\n"
          ]
        }
      ],
      "source": [
        "# Es crucial poner el modelo en modo de evaluación y en la CPU\n",
        "model_fp32_prepared.to('cpu').eval()\n",
        "model_int8 = torch.ao.quantization.convert(model_fp32_prepared, inplace=False)\n",
        "\n",
        "print(\"Exportando el modelo preparado para QAT a ONNX...\")\n",
        "dummy_input = torch.randn(1, 3, 224, 224, device='cpu')\n",
        "onnx_model_path = \"alexnet_qat_int8.onnx\"\n",
        "\n",
        "torch.onnx.export(\n",
        "    model_int8,\n",
        "    dummy_input,\n",
        "    onnx_model_path,\n",
        "    export_params=True,\n",
        "    opset_version=13,\n",
        "    do_constant_folding=True,\n",
        "    input_names=['input'],\n",
        "    output_names=['output'],\n",
        "    fallback=True\n",
        ")\n",
        "print(f\"Modelo exportado a {onnx_model_path} exitosamente.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcIiFXF7ROfO"
      },
      "source": [
        "Además, vamos a realizar una  inferencia de prueba para analizar el rendimiento del modelo inicial y el cuantizado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNE3EyU6JUx4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c70cfbe-7f2d-4fd9-812d-04bbf73352d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 224, 244])\n",
            "Execution time of the fp32 model: 0.129s\n",
            "Execution time of the int8 model: 0.035s\n"
          ]
        }
      ],
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "ONNX_FILE_PATH = \"alexnet_qat_int8.onnx\"\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "print(f\"Versión de ONNX Runtime: {ort.get_device()}\")\n",
        "print(f\"Proveedores disponibles: {ort.get_available_providers()}\")\n",
        "\n",
        "# ONNX Runtime intentará usarlos en el orden especificado.\n",
        "# 'TensorrtExecutionProvider' usará TensorRT para la máxima aceleración en INT8.\n",
        "# 'CUDAExecutionProvider' es un fallback que también usa la GPU.\n",
        "# 'CPUExecutionProvider' se usará si los otros fallan.\n",
        "providers = [\n",
        "    'TensorrtExecutionProvider',\n",
        "    'CUDAExecutionProvider',\n",
        "    'CPUExecutionProvider',\n",
        "]\n",
        "\n",
        "print(f\"\\nCreando sesión de inferencia con los proveedores: {providers}\")\n",
        "session = ort.InferenceSession(ONNX_FILE_PATH, providers=providers)\n",
        "\n",
        "# Obtener los nombres de entrada y salida del modelo\n",
        "input_name = session.get_inputs()[0].name\n",
        "output_name = session.get_outputs()[0].name\n",
        "input_shape = session.get_inputs()[0].shape\n",
        "dummy_preprocessed_image = np.random.rand(BATCH_SIZE, 3, 224, 224).astype(np.float32)\n",
        "\n",
        "print(\"\\nEjecutando inferencia...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# La inferencia se ejecuta con una simple llamada a 'run()'\n",
        "results = session.run([output_name], {input_name: dummy_preprocessed_image})[0]\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Inferencia completada en {end_time - start_time:.4f} segundos.\")\n",
        "\n",
        "import io\n",
        "\n",
        "def get_model_size_in_memory(model):\n",
        "    \"\"\"\n",
        "    Calcula el tamaño del state_dict del modelo en un buffer de memoria.\n",
        "    \"\"\"\n",
        "    # Creamos un buffer en memoria\n",
        "    buffer = io.BytesIO()\n",
        "    # Guardamos el diccionario de estado del modelo en el buffer\n",
        "    torch.save(model.state_dict(), buffer)\n",
        "    # Obtenemos el tamaño del buffer en bytes\n",
        "    size_in_bytes = buffer.getbuffer().nbytes\n",
        "    return size_in_bytes\n",
        "\n",
        "print(\"\\n--- Calculando el tamaño de los modelos en memoria ---\")\n",
        "\n",
        "# 1. Medir el modelo original (FP32)\n",
        "fp32_size = get_model_size_in_memory(model)\n",
        "print(f\"Modelo Original (FP32): {fp32_size / 1e6:.2f} MB\")\n",
        "\n",
        "# 2. Medir el modelo cuantizado (INT8)\n",
        "# Asegúrate de que tienes la variable 'model_int8' o similar\n",
        "int8_size = get_model_size_in_memory(model_int8)\n",
        "print(f\"Modelo Cuantizado (INT8): {int8_size / 1e6:.2f} MB\")\n",
        "\n",
        "# 3. Calcular y mostrar la reducción\n",
        "reduction = 100 * (1 - int8_size / fp32_size)\n",
        "print(f\"Reducción de tamaño: {reduction:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}